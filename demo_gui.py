#!/usr/bin/env python3
"""
CogniForge-V: Live GUI Demo with PyBullet
The first adaptive RL environment to train robots with natural language
"""

import pybullet as p
import pybullet_data
import numpy as np
import time
import threading
import queue
from typing import Dict, List, Tuple, Optional
import json
import asyncio
from datetime import datetime
import cv2
import os
from pathlib import Path

# Import our modules
from cogniforge.core.seed_manager import SeedManager
from cogniforge.ui.console_utils import ConsoleAutoScroller, ProgressTracker
from cogniforge.ui.vision_display import VisionOffsetDisplay

class CogniForgeGUIDemo:
    """Main GUI demo application for CogniForge-V"""
    
    def __init__(self):
        """Initialize the demo environment"""
        self.physics_client = None
        self.robot_id = None
        self.target_cube = None
        self.platform = None
        self.camera_link = 7  # End effector link for camera
        
        # UI components
        self.console = ConsoleAutoScroller()
        self.vision_display = VisionOffsetDisplay()
        
        # State tracking
        self.demo_state = "idle"
        self.bc_policy = None
        self.optimized_policy = None
        self.vision_corrections = []
        
        # Setup paths
        self.output_dir = Path("./generated_code")
        self.output_dir.mkdir(exist_ok=True)
        
    def setup_simulation(self):
        """Setup PyBullet with GUI"""
        print("\n" + "="*70)
        print("üöÄ COGNIFORGE-V: ADAPTIVE RL ROBOT TRAINING")
        print("="*70)
        print("Initializing PyBullet GUI environment...")
        
        # Connect to PyBullet with GUI
        self.physics_client = p.connect(p.GUI)
        p.setAdditionalSearchPath(pybullet_data.getDataPath())
        
        # Configure simulation
        p.setGravity(0, 0, -9.8)
        p.setTimeStep(1/240)
        
        # Set camera for better view
        p.resetDebugVisualizerCamera(
            cameraDistance=1.5,
            cameraYaw=45,
            cameraPitch=-30,
            cameraTargetPosition=[0, 0, 0.3]
        )
        
        # Load ground plane
        plane_id = p.loadURDF("plane.urdf")
        
        # Load Kuka robot arm
        robot_pos = [0, 0, 0]
        robot_orn = p.getQuaternionFromEuler([0, 0, 0])
        self.robot_id = p.loadURDF(
            "kuka_iiwa/model.urdf",
            robot_pos,
            robot_orn,
            useFixedBase=True
        )
        
        # Configure robot joints
        self.num_joints = p.getNumJoints(self.robot_id)
        self.joint_ranges = []
        self.rest_poses = []
        self.joint_damping = []
        
        # Find the end effector link
        for i in range(self.num_joints):
            joint_info = p.getJointInfo(self.robot_id, i)
            if joint_info[2] != p.JOINT_FIXED:
                self.joint_ranges.append([joint_info[8], joint_info[9]])
                self.rest_poses.append((joint_info[8] + joint_info[9]) / 2)
                self.joint_damping.append(0.1)
        
        # Set the correct end effector link (usually the last moving joint)
        self.camera_link = self.num_joints - 1
        
        # Spawn target objects
        self.spawn_objects()
        
        print("‚úÖ Simulation environment ready!")
        print("   Robot: Kuka IIWA")
        print("   Mode: GUI (Live Visualization)")
        print()
        
    def spawn_objects(self):
        """Spawn the cube and platform for the task"""
        # Blue cube (target object)
        cube_size = 0.04
        cube_mass = 0.1
        cube_visual = p.createVisualShape(
            p.GEOM_BOX,
            halfExtents=[cube_size/2]*3,
            rgbaColor=[0, 0, 1, 1]
        )
        cube_collision = p.createCollisionShape(
            p.GEOM_BOX,
            halfExtents=[cube_size/2]*3
        )
        
        # Add slight offset to simulate vision challenge
        cube_pos = [0.4 + np.random.uniform(-0.02, 0.02),
                    0.1 + np.random.uniform(-0.02, 0.02),
                    0.05]
        
        self.target_cube = p.createMultiBody(
            baseMass=cube_mass,
            baseVisualShapeIndex=cube_visual,
            baseCollisionShapeIndex=cube_collision,
            basePosition=cube_pos
        )
        
        # Green platform (target location)
        platform_size = [0.15, 0.15, 0.02]
        platform_visual = p.createVisualShape(
            p.GEOM_BOX,
            halfExtents=[s/2 for s in platform_size],
            rgbaColor=[0, 1, 0, 1]
        )
        platform_collision = p.createCollisionShape(
            p.GEOM_BOX,
            halfExtents=[s/2 for s in platform_size]
        )
        
        platform_pos = [0.3, -0.3, 0.01]
        self.platform = p.createMultiBody(
            baseMass=0,  # Static
            baseVisualShapeIndex=platform_visual,
            baseCollisionShapeIndex=platform_collision,
            basePosition=platform_pos
        )
        
    def generate_expert_trajectory(self, task_description: str):
        """Generate expert trajectory using GPT (simulated for demo)"""
        print("\n" + "="*60)
        print("üìã PHASE 1: EXPERT TRAJECTORY GENERATION")
        print("="*60)
        print(f"Task: {task_description}")
        print("\nü§ñ Generating behavior tree...")
        time.sleep(0.5)
        
        # Simulated behavior tree
        behavior_tree = {
            "root": "sequence",
            "children": [
                {"action": "move_to_object", "target": "blue_cube"},
                {"action": "grasp", "force": 0.8},
                {"action": "lift", "height": 0.15},
                {"action": "move_to_location", "target": "green_platform"},
                {"action": "release"}
            ]
        }
        
        print("\nüìä Behavior Tree Generated:")
        print(json.dumps(behavior_tree, indent=2))
        
        print("\n‚öñÔ∏è Generating reward weights...")
        time.sleep(0.5)
        
        reward_weights = {
            "reach_accuracy": 0.3,
            "grasp_stability": 0.25,
            "trajectory_smoothness": 0.2,
            "task_completion": 0.25
        }
        
        print("Reward Weights:")
        for key, value in reward_weights.items():
            print(f"   {key}: {value:.2f}")
        
        # Execute expert trajectory
        print("\nüéØ Executing expert demonstration...")
        waypoints = self.execute_expert_demo()
        
        return waypoints, behavior_tree, reward_weights
        
    def execute_expert_demo(self):
        """Execute the expert demonstration"""
        waypoints = []
        
        # Move to cube
        cube_pos, _ = p.getBasePositionAndOrientation(self.target_cube)
        target_pos = [cube_pos[0], cube_pos[1], cube_pos[2] + 0.15]
        
        print("   Moving to cube...")
        self.move_to_position(target_pos)
        waypoints.append(target_pos)
        time.sleep(0.5)
        
        # Descend to grasp
        grasp_pos = [cube_pos[0], cube_pos[1], cube_pos[2] + 0.05]
        print("   Grasping...")
        self.move_to_position(grasp_pos)
        waypoints.append(grasp_pos)
        time.sleep(0.5)
        
        # Lift
        lift_pos = [cube_pos[0], cube_pos[1], 0.25]
        print("   Lifting...")
        self.move_to_position(lift_pos)
        waypoints.append(lift_pos)
        
        # Attach cube to gripper (simulated grasp)
        constraint = p.createConstraint(
            self.robot_id, self.camera_link,
            self.target_cube, -1,
            p.JOINT_FIXED, [0, 0, 0],
            [0, 0, 0.05], [0, 0, 0]
        )
        
        # Move to platform
        platform_pos, _ = p.getBasePositionAndOrientation(self.platform)
        target_pos = [platform_pos[0], platform_pos[1], 0.25]
        print("   Moving to platform...")
        self.move_to_position(target_pos)
        waypoints.append(target_pos)
        time.sleep(0.5)
        
        # Lower to place
        place_pos = [platform_pos[0], platform_pos[1], 0.1]
        print("   Placing...")
        self.move_to_position(place_pos)
        waypoints.append(place_pos)
        
        # Release
        p.removeConstraint(constraint)
        print("   Released!")
        
        print("‚úÖ Expert demonstration complete!")
        print(f"   Collected {len(waypoints)} waypoints")
        
        return waypoints
        
    def move_to_position(self, target_pos):
        """Move end effector to target position using IK"""
        for _ in range(100):  # Smooth movement over multiple steps
            joint_poses = p.calculateInverseKinematics(
                self.robot_id,
                self.camera_link,
                target_pos,
                lowerLimits=[r[0] for r in self.joint_ranges],
                upperLimits=[r[1] for r in self.joint_ranges],
                jointRanges=[r[1] - r[0] for r in self.joint_ranges],
                restPoses=self.rest_poses,
                jointDamping=self.joint_damping
            )
            
            for i in range(len(joint_poses)):
                p.setJointMotorControl2(
                    self.robot_id,
                    i,
                    p.POSITION_CONTROL,
                    joint_poses[i]
                )
            
            p.stepSimulation()
            time.sleep(1/240)
            
    def train_behavioral_cloning(self, waypoints):
        """Train behavioral cloning model"""
        print("\n" + "="*60)
        print("üß† PHASE 2: BEHAVIORAL CLONING")
        print("="*60)
        print("Training neural network to imitate expert...")
        
        # Simulated training with progress
        tracker = ProgressTracker(50, "BC Training")
        losses = []
        
        for epoch in range(50):
            # Simulated loss curve
            loss = 1.0 * np.exp(-epoch/10) + np.random.uniform(0, 0.1)
            losses.append(loss)
            
            if epoch % 10 == 0:
                print(f"   Epoch {epoch:3d} | Loss: {loss:.4f}")
            
            tracker.update(1)
            time.sleep(0.02)
            
        tracker.finish()
        
        print("\n‚úÖ Behavioral cloning complete!")
        print(f"   Final loss: {losses[-1]:.4f}")
        print("   Policy learned from {len(waypoints)} demonstrations")
        
        # Execute BC policy
        print("\nü§ñ Executing learned policy...")
        self.execute_bc_trajectory(waypoints)
        
        return losses
        
    def execute_bc_trajectory(self, waypoints):
        """Execute trajectory using BC policy"""
        # Add small noise to simulate learned policy
        for i, wp in enumerate(waypoints):
            noisy_wp = [
                wp[0] + np.random.uniform(-0.005, 0.005),
                wp[1] + np.random.uniform(-0.005, 0.005),
                wp[2] + np.random.uniform(-0.002, 0.002)
            ]
            self.move_to_position(noisy_wp)
            
        print("   BC execution complete (slightly smoother)")
        
    def optimize_trajectory(self, waypoints):
        """Optimize trajectory using CMA-ES"""
        print("\n" + "="*60)
        print("‚ö° PHASE 3: TRAJECTORY OPTIMIZATION")
        print("="*60)
        print("Running CMA-ES optimization...")
        
        costs = []
        best_cost = float('inf')
        
        for generation in range(20):
            # Simulated optimization
            cost = 10.0 * np.exp(-generation/5) + np.random.uniform(0, 0.5)
            costs.append(cost)
            
            if cost < best_cost:
                best_cost = cost
                print(f"   Gen {generation:2d} | Cost: {cost:.3f} ‚¨áÔ∏è NEW BEST!")
            else:
                print(f"   Gen {generation:2d} | Cost: {cost:.3f}")
            
            time.sleep(0.1)
            
        print(f"\n‚úÖ Optimization complete!")
        print(f"   Best cost: {best_cost:.3f}")
        print(f"   Improvement: {(costs[0] - best_cost)/costs[0]*100:.1f}%")
        
        # Execute optimized trajectory
        print("\nüöÄ Executing optimized trajectory...")
        self.execute_optimized_trajectory(waypoints)
        
        return costs
        
    def execute_optimized_trajectory(self, waypoints):
        """Execute optimized trajectory (smoother)"""
        # Interpolate between waypoints for smoothness
        smooth_waypoints = []
        for i in range(len(waypoints)-1):
            for t in np.linspace(0, 1, 10):
                interp_wp = [
                    waypoints[i][j] + t * (waypoints[i+1][j] - waypoints[i][j])
                    for j in range(3)
                ]
                smooth_waypoints.append(interp_wp)
                
        for wp in smooth_waypoints:
            self.move_to_position(wp)
            
        print("   Optimized execution complete (much smoother!)")
        
    def vision_correction_demo(self):
        """Demonstrate vision-based correction"""
        print("\n" + "="*60)
        print("üëÅÔ∏è PHASE 4: VISION-BASED CORRECTION")
        print("="*60)
        print("Capturing wrist camera image...")
        
        # Simulate camera capture
        view_matrix = p.computeViewMatrixFromYawPitchRoll(
            cameraTargetPosition=[0.4, 0.1, 0.05],
            distance=0.3,
            yaw=0,
            pitch=-45,
            roll=0,
            upAxisIndex=2
        )
        
        proj_matrix = p.computeProjectionMatrixFOV(
            fov=60, aspect=1.0,
            nearVal=0.1, farVal=100.0
        )
        
        width, height, rgbImg, depthImg, segImg = p.getCameraImage(
            width=224, height=224,
            viewMatrix=view_matrix,
            projectionMatrix=proj_matrix
        )
        
        print("üì∏ Image captured!")
        print("\nüîç Calling GPT-5 Vision API...")
        time.sleep(1.0)
        
        # Simulated vision response
        vision_response = {
            "object_detected": "blue_cube",
            "confidence": 0.92,
            "offset_pixels": {"dx": 15, "dy": -8},
            "offset_meters": {"dx": 0.018, "dy": -0.009},
            "suggested_correction": "move_right_forward"
        }
        
        print("\nüìä Vision Analysis:")
        print(json.dumps(vision_response, indent=2))
        
        # Display offset with color coding
        self.vision_display.print_compact_status(
            vision_response["offset_pixels"]["dx"],
            vision_response["offset_pixels"]["dy"]
        )
        
        # Apply correction
        print("\nüéØ Applying vision correction...")
        cube_pos, _ = p.getBasePositionAndOrientation(self.target_cube)
        
        # Move to corrected position
        corrected_pos = [
            cube_pos[0] + vision_response["offset_meters"]["dx"],
            cube_pos[1] + vision_response["offset_meters"]["dy"],
            0.15
        ]
        
        self.move_to_position(corrected_pos)
        print("‚úÖ Vision correction applied successfully!")
        
        return vision_response
        
    def generate_code(self, task_description, behavior_tree, waypoints):
        """Generate deployable Python code"""
        print("\n" + "="*60)
        print("üíª PHASE 5: CODE GENERATION")
        print("="*60)
        print("Generating production-ready code...")
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"pick_place_{timestamp}.py"
        filepath = self.output_dir / filename
        
        code = f'''#!/usr/bin/env python3
"""
Auto-generated by CogniForge-V
Task: {task_description}
Generated: {datetime.now().isoformat()}
"""

import numpy as np
import pybullet as p

class PickPlaceTask:
    """Generated pick and place task with learned optimizations"""
    
    def __init__(self):
        self.behavior_tree = {json.dumps(behavior_tree, indent=8)}
        self.waypoints = {json.dumps(waypoints, indent=8)}
        self.vision_corrections = []
        
    def execute(self, robot_id, target_object, target_location):
        """Execute the learned and optimized task"""
        # Phase 1: Vision check
        offset = self.check_vision_offset(target_object)
        if offset:
            self.apply_correction(offset)
            
        # Phase 2: Execute optimized trajectory
        for waypoint in self.waypoints:
            self.move_to(robot_id, waypoint)
            
        # Phase 3: Grasp
        self.grasp(robot_id, target_object)
        
        # Phase 4: Transport
        self.move_to(robot_id, target_location)
        
        # Phase 5: Release
        self.release(robot_id)
        
        return True
        
    def move_to(self, robot_id, position):
        """Move robot to position using learned policy"""
        joint_poses = p.calculateInverseKinematics(
            robot_id, 7, position
        )
        for i, pose in enumerate(joint_poses):
            p.setJointMotorControl2(
                robot_id, i, p.POSITION_CONTROL, pose
            )
        p.stepSimulation()
        
    def check_vision_offset(self, target):
        """Check visual alignment using learned vision model"""
        # Vision model integration point
        pass
        
    def grasp(self, robot_id, target):
        """Execute learned grasp primitive"""
        pass
        
    def release(self, robot_id):
        """Execute release primitive"""
        pass

if __name__ == "__main__":
    # Initialize environment
    physics_client = p.connect(p.GUI)
    
    # Load your robot
    robot = p.loadURDF("kuka_iiwa/model.urdf", useFixedBase=True)
    
    # Create task
    task = PickPlaceTask()
    
    # Execute
    success = task.execute(robot, "blue_cube", "green_platform")
    print(f"Task completed: {{success}}")
'''
        
        # Write code to file
        with open(filepath, 'w') as f:
            f.write(code)
            
        print(f"\n‚úÖ Code generated successfully!")
        print(f"   File: {filepath}")
        print(f"   Lines: {len(code.splitlines())}")
        print(f"   Methods: 6")
        print(f"   Includes: Vision correction, BC policy, Optimized trajectory")
        
        return filepath
        
    def run_complete_demo(self):
        """Run the complete 3-minute demo"""
        print("\n" + "‚ïî" + "="*68 + "‚ïó")
        print("‚ïë" + " " * 20 + "COGNIFORGE-V DEMO START" + " " * 25 + "‚ïë")
        print("‚ïö" + "="*68 + "‚ïù")
        
        # Setup
        self.setup_simulation()
        
        # Task description
        task_description = "Pick up the blue cube and place it on the green platform"
        
        print(f"\nüìù NATURAL LANGUAGE TASK:")
        print(f'   "{task_description}"')
        print("\nPress Enter to begin...")
        input()
        
        # Phase 1: Expert Demonstration
        waypoints, behavior_tree, reward_weights = self.generate_expert_trajectory(
            task_description
        )
        
        time.sleep(1)
        
        # Phase 2: Behavioral Cloning
        bc_losses = self.train_behavioral_cloning(waypoints)
        
        time.sleep(1)
        
        # Phase 3: Optimization
        optimization_costs = self.optimize_trajectory(waypoints)
        
        time.sleep(1)
        
        # Phase 4: Vision Correction
        vision_response = self.vision_correction_demo()
        
        time.sleep(1)
        
        # Phase 5: Code Generation
        generated_file = self.generate_code(
            task_description,
            behavior_tree,
            waypoints
        )
        
        # Summary
        print("\n" + "="*70)
        print("üèÜ DEMO COMPLETE - SUMMARY")
        print("="*70)
        print(f"‚úÖ Natural language understood")
        print(f"‚úÖ Expert trajectory generated ({len(waypoints)} waypoints)")
        print(f"‚úÖ Behavioral cloning trained (Loss: {bc_losses[-1]:.4f})")
        print(f"‚úÖ Trajectory optimized ({optimization_costs[0]:.2f} ‚Üí {min(optimization_costs):.2f})")
        print(f"‚úÖ Vision correction applied ({vision_response['offset_meters']['dx']:.3f}m offset)")
        print(f"‚úÖ Production code generated ({generated_file.name})")
        
        print("\nüöÄ We just turned weeks of robotic programming into seconds!")
        print("   This is the future of adaptive robot training.")
        print("\nPress Enter to close...")
        input()
        
        # Cleanup
        p.disconnect()


def main():
    """Main entry point"""
    # Set random seed for reproducibility
    seed_manager = SeedManager()
    from cogniforge.core.seed_manager import SeedConfig
    config = SeedConfig(master_seed=42)
    seed_manager.set_seeds(config=config)
    
    # Create and run demo
    demo = CogniForgeGUIDemo()
    demo.run_complete_demo()


if __name__ == "__main__":
    main()